---
title: "Hypothesis Testing 1"
subtitle: "Single Proportion"
date: today
date-format: long
footer: "[IAFF 6501 Website](https://quant4ia.rocks/)"
logo: images/iaff6501-logo.png
format:
  revealjs:
    theme: [simple, custom.scss]
    transition: fade
    slide-number: true
    #multiplex: true
    chalkboard: true
execute:
  echo: false
  message: false
  warning: false
  freeze: auto
---

## Jobs Training Programs

-   International development organizations are sometimes interested in providing training to people in order to help them find a job
-   Imagine the unemployment rate in a low-income country is 30%
-   One organization claims that its jobs training program is a success because only 15 of the 60 people that they trained did not have a job (25% unemployment rate)
-   What should we think about this claim? Is this a successful program?

## Today's Code

<https://www.dropbox.com/scl/fo/k6q8n5lbwbff6j0h5vvs1/AK9Ibhnd8t3rxV5FneYf8AU?rlkey=zo2rejko587tfvoq8tterd44r&dl=0>

## 

<br>

Now let's create some data to match our hypothetical example.

<br>

```{r}
#| echo: true

library(tidyverse)

jobs_program <- tibble(
  outcome = c(rep("unemployed", 15), rep("employed", 45))
)

```

## 

<br>

Use the base R `head()` function to see the first five rows.

```{r}
#| echo: true

head(jobs_program)
```

## 

<br>

Use the `tail()` function to see the last five rows.

```{r}
#| echo: true

tail(jobs_program)
```

## 

<br>

Now let's visualize it with a bar chart.

```{r}
jobs_program %>%
  ggplot(aes(x = outcome)) +
  geom_bar(fill = "steelblue") + theme_bw()
```

## Question

<br>

Is it possible to assess this hypothetical organization's claim using the data and information presented thus far?

>"Our jobs program is a success because only 15 of the 60 people that we trained did not have a job. Thus our 25% unemployment rate beats the country's unemployment rate of 30%."

## Correlation vs. causation

<br>

-   No.
- We need to know more about how people were selected for the program in order to assess causality (e.g., were they randomly assigned?)
-   But, we can still ask whether the unemployment rate of $\hat{p}$ = `r round(15/60, 3)` could be due to chance.

## Hypothesis Testing Intuition {.smaller}

<br>

- We are going to assume "nothing is going on"
    - In this case, the jobs program had no impact

. . .

- We are going to figure out what the distribution of outcomes we we might observe could be if nothing is going on
    - In this case: if we take a sample of 60 from a population where the parameter is 0.3

. . .

- We will assess how likely we would be to observe our data if nothing is going on
    - If very unlikely, we conclude that something is probably going on

## Stating our Hypotheses

<br>

**Null hypothesis ($H_0$):** "There is nothing going on."

> Unemployment rate among those in the jobs program is no different than the country average of 30%.

. . .

**Alternative hypothesis ($H_A$):** "There is something going on."

> Unemployment rate is **lower** than the country average of 30%.


## Hypothesis Test

<br>

::: incremental
- **Hypothesis test:** If the null hypothesis were true, is the data we have in our sample likely to have been generated by chance (due to random variability)?
- If yes, we do NOT reject the null hypothesis
- If not very likely, we reject the null hypothesis 
:::

## Hypothesis Testing Framework 

::: incremental
-   Start with null hypothesis, $H_0$, represents the status quo
-   Set an alternative hypothesis, $H_A$, that represents the research question, i.e. what we're testing for
-   Conduct a hypothesis test under the assumption that the null hypothesis is true 
    -   if the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis
    -   if they do, then reject the null hypothesis in favor of the alternative
:::

## p-values and Critical Values

<br>

::: incremental
- A **p-value** is the probability of observed or more extreme outcome given that the null hypothesis is true
- A critical value ($\alpha$) is the threshold at which we will reject the null hypothesis
- If the p-value is less than $\alpha$, we reject the null hypothesis
- A standard threshold for $\alpha$ is 0.05
:::

## The Null Distribution

<br>

-   Since $H_0: p = 0.30$, we need to simulate a null distribution where the probability of success (unemployment) for each trial (person in program) is 0.30

. . .

-   We want to know how likely we would be to get an unemployment rate of 0.25 in our sample of 60, *if the true unemployment rate were 0.30*


## What do we expect?

<br>

::: incremental
- So the first step is to simulate our null distribution
- And the question is, when sampling from the null distribution, what is the expected proportion of unemployed?
- We set up our simulator to select samples of 60 individuals with a 30% chance of being unemployed
- We then calculate the proportion of unemployed in each sample
:::

## Simulation #1

```{r} 
#| out-width: 50%

# set seed
set.seed(12112021)
# create sample space
outcomes <- c("unemployed", "employed")
# draw the first sample of size 60 from the null distribution
sim1 <- sample(outcomes, size = 60, prob = c(0.3, 0.7), replace = TRUE)
# view the sample
table(sim1)
# calculate the simulated sample proportion 
(p_hat_sim1 <- sum(sim1 == "unemployed") / length(sim1))

# create an empty data frame
sim_dist <- data.frame(p_hat_sim = rep(NA, 3))

# record the simulated p-hat as the first observation
sim_dist$p_hat_sim[1] <- p_hat_sim1

# plot
ggplot(sim_dist, aes(x = p_hat_sim)) + 
  geom_dotplot() + 
  xlim(0, 0.5) + ylim(0, 10)
```

------------------------------------------------------------------------

## Simulation #2

```{r}
#| out-width: 50%

sim2 <- sample(outcomes, size = 60, prob = c(0.3, 0.7), replace = TRUE)

table(sim2)

(p_hat_sim2 <- sum(sim2 == "unemployed") / length(sim2))

sim_dist$p_hat_sim[2] <- p_hat_sim2

ggplot(sim_dist, aes(x = p_hat_sim)) + 
  geom_dotplot() + 
  xlim(0,0.5) + ylim(0,10)
```

## Simulation #3

```{r}
#| out-width: 50%

sim3 <- sample(outcomes, size = 60, prob = c(0.3, 0.7), replace = TRUE)

table(sim3)

(p_hat_sim3 <- sum(sim3 == "unemployed") / length(sim3))

sim_dist$p_hat_sim[3] <- p_hat_sim3

ggplot(sim_dist, aes(x = p_hat_sim)) + 
  geom_dotplot() + 
  xlim(0,0.5) + ylim(0,10)
```

## We need to do this many times...

## `tidymodels`

<br>

We can use the `tidymodels` package to help with this process...

```{r}
#| echo: true

#load tidymodels
library(tidymodels)

# simulate the distribution
null_dist <- jobs_program |>
  specify(response = outcome, success = "unemployed") |>
  hypothesize(null = "point", p = c("unemployed" = 0.30, "employed" = 0.70)) |>
  generate(reps = 2000, type = "draw") |> 
  calculate(stat = "prop")
```

## What is being stored in *null_dist*?

<br>

```{r}
null_dist |>
  mutate(
    replicate = as.numeric(replicate),
    stat = round(stat, 3)
    )
```

## The null distribution

<br>

Where should this distribution be centered?  Or, what should the mean be?

<br>

. . .

```{r}
#| echo: true 

null_dist |>
  summarize(mean = mean(stat))
```

## Visualizing the null distribution

```{r out.width="40%"}
ggplot(data = null_dist, mapping = aes(x = stat)) +
  geom_histogram(binwidth = 0.05, fill = "steelblue4") +
  labs(title = "Null distribution")  + theme_bw()
```

## Calculate the p-value

<br> 
*p-value*--in what % of the simulations was the simulated sample proportion at least as extreme as the observed sample proportion?

```{r}
#| echo: true

null_dist |>
  # select out the value in the null distribution that are less that 0.25
  filter(stat <= (15/60)) |>
  # calculate the proportion - (number less divided by all values in null_dist)
  summarise(p_value = n()/nrow(null_dist))
```

## Visualizing the p-value

```{r echo=FALSE, out.width="50%"}
ggplot(data = null_dist, mapping = aes(x = stat)) +
  geom_histogram(binwidth = 0.05, fill = "steelblue4") +
  labs(title = "Null distribution") +     
  geom_vline(xintercept = .25, linetype="dotted", 
                color = "black", size=1) + theme_bw()
```

## p-value using `infer`

<br>

```{r}
#| echo: true

library(infer)

# calculate the p-value from null distribution
p_value <- null_dist |>
  get_p_value(obs_stat = 15/60, direction = "less")

p_value
```

## Visualing the p-value using `infer`

<br>

```{r}
#| echo: true

visualize(null_dist) +
  shade_p_value(obs_stat = 15/60, direction = "less")
```


## "Significance" level {.smaller}

<br>

::: incremental
- Conventionally, people use a p-value of *0.05* as a cutoff ("signifigance level") for determining "statistical significance"
    - That is, whether the null hypothesis should be rejected
    - That is, whether the data we gathered is very unlikely to have been generated due to chance
- Always remember that this is a convention
    - *p=0.049* is under the cutoff, while p=0.051 is not: are these really different?
- When people report "statistically significant" results, they mean that the p-value from their analysis is less than 0.05
:::

## Our Hypothetical Study

<br>

- Our finding: if the true unemployment rate were 30 percent and we draw samples of 60, about 23 percent of the time we will get an unemployment rate lower than the one among the participants in the program (simply due to random chance)
- What should we conclude?

## Conclusion

<br>

- We do NOT reject the null hypothesis: the unemployment rate in the sample could likely have been due to chance

## Your Turn!

- What if the unemployment rate for the program was only 10%?     - Would you reject the null hypothesis in this case?
    - Demonstrate by calculating the p-value
- Try changing the true unemployment rate in the null distribution to 0.50 (50%)
  - Simulate the null distribution
  - Would you reject the null hypothesis if the observed unemployment rate was 23% in this case?

